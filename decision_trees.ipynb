{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees Machine Learning Project\n",
    "\n",
    "In this project, we explore and implement a Decision Trees model using real-world data. We will work with the Titanic dataset to predict survival, perform exploratory data analysis, preprocess the data, and analyze the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f10f8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Decision Trees are a popular machine learning algorithm used for classification and regression tasks. They work by recursively splitting the dataset into subsets based on feature values. The simplicity and interpretability of decision trees make them a valuable tool for understanding data patterns and making predictions.\n",
    "\n",
    "Objectives of this project:\n",
    "- Understand the mechanism behind Decision Trees, including entropy, information gain, and Gini impurity.\n",
    "- Perform exploratory data analysis on the dataset to uncover underlying patterns.\n",
    "- Preprocess the data and prepare it for machine learning modeling.\n",
    "- Train and evaluate a Decision Tree classifier using real-world data.\n",
    "- Visualize the decision path and understand feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all warning messages that would normally be displayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set(style='whitegrid')\n",
    "# Display plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbc9db",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The dataset used in this project is the Titanic dataset from the Seaborn library. It contains comprehensive details about the passengers aboard the Titanic, which include the following key features:\n",
    "\n",
    "- **pclass**: The passenger class (1st, 2nd, 3rd), representing the socio-economic status.\n",
    "- **sex**: The gender of the passenger.\n",
    "- **age**: The age of the passenger.\n",
    "- **sibsp**: The number of siblings or spouses aboard the ship.\n",
    "- **parch**: The number of parents or children aboard the ship.\n",
    "- **fare**: The ticket fare paid by the passenger.\n",
    "- **embarked**: The port where the passenger boarded (C = Cherbourg; Q = Queenstown; S = Southampton).\n",
    "\n",
    "The target variable is **survived**, which indicates whether the passenger survived (1) or did not survive (0). This dataset provides valuable insights into the demographics and conditions that could have influenced survival, making it an excellent candidate for applying Decision Trees to classify the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset from seaborn\n",
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the raw dataset\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e707ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05bca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc38bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of continuous features\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(titanic['age'].dropna(), kde=True, bins=30)\n",
    "plt.title('Age Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(titanic['fare'], kde=True, bins=30)\n",
    "plt.title('Fare Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot for survival\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='survived', data=titanic)\n",
    "plt.title('Survival Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea18ab1b",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we preprocess the data to prepare it for modeling. We handle missing values, encode categorical variables, and split the data into training and testing sets. For this project, we'll focus on the following features:\n",
    "- pclass\n",
    "- sex\n",
    "- age\n",
    "- sibsp\n",
    "- parch\n",
    "- fare\n",
    "- embarked\n",
    "\n",
    "Our target variable will be 'survived'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e894fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of columns for the model\n",
    "columns_to_use = ['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
    "data = titanic[columns_to_use].copy()\n",
    "\n",
    "# Handle missing values\n",
    "data['age'].fillna(data['age'].median(), inplace=True)  # Fill missing ages with median\n",
    "data['embarked'].fillna(data['embarked'].mode()[0], inplace=True)  # Fill missing embarked with mode (most frequent value)\n",
    "\n",
    "# Convert categorical features into dummy/indicator variables\n",
    "data_encoded = pd.get_dummies(data, columns=['sex', 'embarked'], drop_first=True)\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature matrix X and target vector y\n",
    "X = data_encoded.drop('survived', axis=1)\n",
    "y = data_encoded['survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print('Training set size:', X_train.shape)\n",
    "print('Testing set size:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation of Decision Trees\n",
    "\n",
    "Decision Trees classify data by splitting it based on feature values, creating branches until a decision (or leaf node) is reached. Some key concepts include:\n",
    "\n",
    "- Entropy: A measure of the randomness or impurity in the data. It is defined as:\n",
    "  $E = -Σ p(i) log₂ p(i)$\n",
    "\n",
    "- Information Gain: The reduction in entropy after a dataset is split on an attribute. It is calculated as:\n",
    "  $Information Gain = Entropy(Parent) - Σ (weighted average) Entropy(Child)$\n",
    "\n",
    "- Gini Impurity: Another measure of impurity, calculated as:\n",
    "  $Gini = 1 - Σ p(i)²$\n",
    "\n",
    "The tree splitting is based on selecting the feature and threshold that best separates the data to reduce impurity (using entropy or Gini impurity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('Best Parameters:', grid_search.best_params_)\n",
    "print('Best Cross-Validation Accuracy:', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best estimator to predict on the test set\n",
    "best_dt = grid_search.best_estimator_\n",
    "y_pred_best = best_dt.predict(X_test)\n",
    "print('\\nAccuracy of Best Model:', accuracy_score(y_test, y_pred_best))\n",
    "print('\\nClassification Report (Best Model):\\n', classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62148143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Decision Tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(best_dt, feature_names=X.columns, class_names=['Not Survived', 'Survived'], filled=True, rounded=True, fontsize=10)\n",
    "plt.title('Optimized Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "importances = best_dt.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x=importances[indices], y=X.columns[indices], palette='viridis')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The Decision Tree model provides a straightforward approach to classification, with the interpretability of its decision rules being one of its main advantages.\n",
    "\n",
    "Key observations:\n",
    "- The model achieved reasonable accuracy and the classification report shows the precision, recall, and F1-score for each class.\n",
    "- Hyperparameter tuning helped in finding the optimal complexity of the tree, balancing between underfitting and overfitting.\n",
    "- Feature importance visualization shows which features contributed most to the prediction of survival.\n",
    "\n",
    "Potential limitations:\n",
    "- Decision Trees are prone to overfitting if not properly pruned or tuned.\n",
    "- The model performance might be improved further with ensemble methods such as Random Forests or Gradient Boosted Trees.\n",
    "\n",
    "Future improvements could include more advanced feature engineering, testing additional algorithms, and employing ensemble techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we successfully implemented a Decision Trees model to predict Titanic survival. We walked through the data loading, preprocessing, exploration, model training, hyperparameter tuning, and evaluation processes. Through effective visualization of the decision tree structure and feature importances, we gained insights into how the model makes predictions.\n",
    "\n",
    "This project highlights the importance of data preprocessing, model tuning, and thorough analysis in building robust machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Seaborn Titanic dataset: https://github.com/mwaskom/seaborn-data\n",
    "- Scikit-learn documentation: https://scikit-learn.org/stable/\n",
    "- Decision Trees concepts: https://en.wikipedia.org/wiki/Decision_tree_learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
